# -*- coding: utf-8 -*-
"""EMNIST CLASSIFIER.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SiBXCKq3qSotSIu5K25smAuGxY-QOO25

# Importing Libraries
"""

!pip install -Uqq fastbook
import fastbook
from fastbook import *

fastbook.setup_book()

import fastai
from fastai import *
from fastai.vision.all import *

"""# Downloading and Extracting Data from Kaggle

"""

import os
os.environ['KAGGLE_CONFIG_DIR'] = '/content/gdrive/MyDrive/kaggle'

os.mkdir("EMNIST")

os.chdir("/content/EMNIST")

!kaggle datasets download -d crawford/emnist

!unzip "/content/EMNIST/emnist.zip"

"""# Imporing data and Preprocessing

"""

train_df=pd.read_csv("/content/EMNIST/emnist-balanced-train.csv")
test_df=pd.read_csv("/content/EMNIST/emnist-balanced-test.csv")
train_df.shape,test_df.shape

train_df.head()

test_df.head()

names=["0","1","2","3","4","5","6","7","8","9","A","B","C","D","E","F","G","H","I",
       "J","K","L","M","N","O","P","Q","R","S","T","U","V","W","X","Y","Z","a","b",
      "d","e","f","g","h","n","q","r","t"]

for i in range(len(train_df["45"].unique())):
    print(i)
    train_df.update(train_df.loc[np.where(train_df["45"]==i)]["45"].replace(i,names[i]))

for i in range(len(test_df["41"].unique())):
    print(i)
    test_df.update(test_df.loc[np.where(test_df["41"]==i)]["41"].replace(i,names[i]))

test_df["41"].unique()

train_df["45"].unique()

len(test_df["41"].unique()),len(train_df["45"].unique())

test_df.head()

train_df.head()

"""# Saving processed data as pickel"""

path=Path("/content/EMNIST")
save_pickle(path/"processed_test_df.pkl",test_df)

path=Path("/content/EMNIST")
save_pickle(path/"processed_train_df.pkl",train_df)

"""# Loading data from pickel File"""

train_df=load_pickle("/content/gdrive/MyDrive/EMNIST project/processed_train_df.pkl")
train_df.sort_values(by=["45"],inplace=True)
train_df.head()

test_df=load_pickle("/content/gdrive/MyDrive/EMNIST project/processed_test_df.pkl")
test_df.sort_values(by=["41"],inplace=True)
test_df.head()

train_df.insert(1,"is_valid",0)
test_df.insert(1,"is_valid",1)

train_df

test_df

train_df.rename(columns={'45': 'Class'},inplace=True)
test_df.rename(columns={'41': 'Class'},inplace=True)

df=pd.concat([train_df,test_df],axis=0)
df.head(5)

df.shape,train_df.shape[0]+test_df.shape[0],train_df.shape[1],test_df.shape[1]

train_df.columns[2:]

renaming={}
for i,j in enumerate(train_df.columns[2:]):
      renaming[j]=i

train_df.rename(columns=renaming,inplace=True)

renaming={}
for i,j in enumerate(test_df.columns[2:]):
      renaming[j]=i

test_df.rename(columns=renaming,inplace=True)

df=pd.concat([train_df,test_df],axis=0)
# df.head(5)

df.shape

df

pwd

os.mkdir("EMNSIT")

path=Path("/content/EMNIST")
save_pickle("whole_dataset1.pkl",df)

df=load_pickle("/content/gdrive/MyDrive/EMNIST project/whole_dataset1.pkl")
df

"""# Constructing DataBlock and dataloaders"""

def get_x(r):
    return r.iloc[1:].to_numpy().reshape(28,28).transpose().astype(np.uint8)

def get_y(r):
    return r.iloc[0]

dblock=DataBlock(
        blocks=(ImageBlock(cls=PILImageBW),CategoryBlock),
        get_x=get_x,
        get_y=get_y
)

dls=dblock.dataloaders(train_df)

dls.train.show_batch(max_n=72,nrows=9,ncols=8,cmap="gray")

# dls.valid.show_batch(,max_n=72,nrows=9,ncols=8,cmap="gray")

"""# making learner and training the model using transfer learning and interpreting the model using CONFUSION matrix"""

learn=cnn_learner(dls,resnet34,metrics=accuracy)

learn.summary()

learn.fine_tune(1)

interp = ClassificationInterpretation.from_learner(learn)
interp.plot_confusion_matrix(figsize=(12,12), dpi=60)

interp.most_confused(min_val=5)

"""# with batch size 512"""

def get_x(r):
    return r.iloc[2:].to_numpy().reshape(28,28).transpose().astype(np.uint8)

def get_y(r):
    return r.iloc[0]

MNIST=DataBlock(
        blocks=(ImageBlock(cls=PILImageBW),CategoryBlock),
        get_x=get_x,
        get_y=get_y,
        splitter=ColSplitter(col="is_valid")
 )

dls=MNIST.dataloaders(df,bs=512)

list(dls)

learn=cnn_learner(dls,resnet34,metrics=accuracy)

learn.summary()

learn.lr_find()

learn.fine_tune(10)



















"""# Extracting images from CSV of each class"""

show_images

doc(show_images)

images=[]
names=[]

for i in train_df["45"].unique():
    for j in range(5):
          images.append(train_df.iloc[np.where(train_df["45"]==i)].iloc[j,1:].to_numpy().reshape(28,28).transpose().astype(np.uint8))
          names.append(f"{train_df.iloc[np.where(train_df['45']==i)].iloc[j,0]}_{j}_image")

len(images),len(names)

show_images(images,titles=names,imsize=3,nrows=12,ncols=20)

train_df.iloc[:,0].unique()

images_single=[]
names_single=[]

for i in train_df.iloc[:,0].unique():
        images_single.append(train_df.iloc[np.where(train_df.iloc[:,0]==i)].iloc[0,1:].to_numpy().reshape(28,28).transpose().astype(np.uint8))
        names_single.append(f"{train_df.iloc[np.where(train_df.iloc[:,0]==i)].iloc[0,0]}_{j}_image")

len(images_single),len(names_single)

show_images(images_single,titles=names_single,imsize=3,nrows=5,ncols=10)

a_3_image=[tensor(train_df.iloc[np.where(train_df.iloc[:,0]=="3")].iloc[2,1:].to_numpy().reshape(28,28).transpose().astype(np.uint8)),
           train_df.iloc[np.where(train_df.iloc[:,0]=="3")].iloc[2,0]]
a_H_image=[tensor(train_df.iloc[np.where(train_df.iloc[:,0]=="H")].iloc[3,1:].to_numpy().reshape(28,28).transpose().astype(np.uint8)),
           train_df.iloc[np.where(train_df.iloc[:,0]=="3")].iloc[3,0]]

show_images([a_3_image[0],a_H_image[0]],titles=(a_3_image[1],a_H_image[1]))

"""# exploring KERNEL"""

a_3_image=[tensor(train_df.iloc[np.where(train_df.iloc[:,0]=="3")].iloc[2,1:].to_numpy().reshape(28,28).transpose().astype(np.uint8)),
           train_df.iloc[np.where(train_df.iloc[:,0]=="3")].iloc[2,0]]
a_H_image=[tensor(train_df.iloc[np.where(train_df.iloc[:,0]=="H")].iloc[3,1:].to_numpy().reshape(28,28).transpose().astype(np.uint8)),
           train_df.iloc[np.where(train_df.iloc[:,0]=="3")].iloc[3,0]]

show_images([a_3_image[0],a_H_image[0]],titles=(a_3_image[1],a_H_image[1]))

df = pd.DataFrame(a_3_image[0])
df.style.set_properties(**{'font-size':'6pt'}).background_gradient('Greys')

top_edge = tensor([[-1,-1,-1],
                    [ 0, 0, 0],
                    [ 1, 1, 1]]).float()

def apply_kernel(image,row, col, kernel):
 return (image[row-1:row+2,col-1:col+2] * kernel).sum()

apply_kernel(a_3_image[0],2,8,top_edge)

apply_kernel(a_3_image[0],5,8,top_edge)

rng = range(1,27)
top_edge3 = tensor([[apply_kernel(a_3_image[0],i,j,top_edge) for j in rng] for i in rng])
show_image(top_edge3,cmap="binary",figsize=(3,3));

df = pd.DataFrame(a_H_image[0])
df.style.set_properties(**{'font-size':'6pt'}).background_gradient('Greys')

apply_kernel(a_H_image[0],14,13,top_edge)

apply_kernel(a_H_image[0],16,13,top_edge)

top_edgeH = tensor([[apply_kernel(a_H_image[0],i,j,top_edge) for j in rng] for i in rng])
show_image(top_edgeH,cmap="binary",figsize=(3,3));

left_edge = tensor([[-1,1,0],
                    [-1,1,0],
                    [-1,1,0]]).float()
diag1_edge = tensor([[ 0,-1, 1],
                     [-1, 1, 0],
                     [ 1, 0, 0]]).float()
diag2_edge = tensor([[ 1,-1, 0],
                    [ 0, 1,-1],
                    [ 0, 0, 1]]).float()

top_edge3 = tensor([[apply_kernel(a_3_image[0],i,j,top_edge) for j in rng] for i in rng])
left_edge3 =tensor([[apply_kernel(a_3_image[0],i,j,left_edge) for j in rng] for i in rng])
diag1_edge3 =tensor([[apply_kernel(a_3_image[0],i,j,diag1_edge) for j in rng] for i in rng])
diag2_edge3 =tensor([[apply_kernel(a_3_image[0],i,j,diag2_edge) for j in rng] for i in rng])

show_images([top_edge3,left_edge3,diag1_edge3,diag2_edge3],titles=("top_edge3","left_edge3","diag1_edge3","diag2_edge3"))

top_edgeH = tensor([[apply_kernel(a_H_image[0],i,j,top_edge) for j in rng] for i in rng])
left_edgeH =tensor([[apply_kernel(a_H_image[0],i,j,left_edge) for j in rng] for i in rng])
diag1_edgeH =tensor([[apply_kernel(a_H_image[0],i,j,diag1_edge) for j in rng] for i in rng])
diag2_edgeH =tensor([[apply_kernel(a_H_image[0],i,j,diag2_edge) for j in rng] for i in rng])

show_images([top_edgeH,left_edgeH,diag1_edgeH,diag2_edgeH],titles=("top_edge3","left_edge3","diag1_edge3","diag2_edge3"))

# show_image(top_edgeH,cmap="binary",figsize=(3,3));
# show_image(left_edgeH,cmap="binary",figsize=(3,3));
# show_image(diag1_edgeH,cmap="binary",figsize=(3,3));
# show_image(diag2_edgeH,cmap="binary",figsize=(3,3));



"""# CNN from scratch"""



def get_x(r):
    return r.iloc[1:].to_numpy().reshape(28,28).transpose().astype(np.uint8)

def get_y(r):
    return r.iloc[0]

dblock=DataBlock(
        blocks=(ImageBlock(cls=PILImageBW),CategoryBlock),
        get_x=get_x,
        get_y=get_y
)

dls=dblock.dataloaders(train_df)

broken_cnn = sequential(
 nn.Conv2d(1,30, kernel_size=3, padding=1),
 nn.ReLU(),
 nn.Conv2d(30,1, kernel_size=3, padding=1)
)

# broken_cnn(a_3_image[0]).shape



simple_net = nn.Sequential(
 nn.Linear(28*28,30),
 nn.ReLU(),
 nn.Linear(30,1)
)

learn=Learner(dls,simple_net,loss_func=F.cross_entropy,metrics=accuracy)

learn.fit(1)

len(dls.vocab)

def conv(ni, nf, ks=3, act=True):
      res = nn.Conv2d(ni, nf, stride=2, kernel_size=ks, padding=ks//2)
      if act: res = nn.Sequential(res, nn.ReLU())
      return res

def simple_cnn():
    return sequential(
            conv(1 ,4),  #14x14
            conv(4 ,8),  #7x7
            conv(8 ,16), #4x4
            conv(16,32), #2x2
            conv(32,47, act=False), #1x1
            Flatten(),
)

learn = Learner(dls, simple_cnn(), loss_func=F.cross_entropy, metrics=accuracy)

learn.summary()

learn.fit_one_cycle(2, 0.01)

CUDA_LAUNCH_BLOCKING=1

def get_dls(bs=64):
  return DataBlock(
        blocks=(ImageBlock(cls=PILImageBW),CategoryBlock),
        get_x=get_x,
        get_y=get_y,
        batch_tfms=Normalize()
  ).dataloaders(train_df, bs=bs)

dls = get_dls()

list(dls)

x,y=dls.one_batch()

x.shape

def conv(ni, nf, ks=3, act=True):
 res = nn.Conv2d(ni, nf, stride=2, kernel_size=ks, padding=ks//2)
 if act: res = nn.Sequential(res, nn.ReLU())
 return res

def simple_cnn():
 return sequential(
 conv(1 ,8, ks=5), #14x14
 conv(8 ,16), #7x7
 conv(16,32), #4x4
 conv(32,64), #2x2
 conv(64,10, act=False), #1x1
 Flatten(),
 )

from fastai.callback.hook import *

def fit(epochs=1):
 learn = Learner(dls, simple_cnn(), loss_func=F.cross_entropy,
 metrics=accuracy, cbs=ActivationStats(with_hist=True))
 learn.fit(epochs, 0.06)
 return learn

learn = fit()

learn=cnn_learner(dls,resnet34,metrics=accuracy)

learn.fine_tune(1)

learn=cnn_learner(dls,resnet34)

learn.fine_tune(1)



"""# ANother attempt fot CNN from scratch"""

def get_x(r):
    return r.iloc[2:].to_numpy().reshape(28,28).transpose().astype(np.uint8)

def get_y(r):
    return r.iloc[0]

MNIST=DataBlock(
        blocks=(ImageBlock(cls=PILImageBW),CategoryBlock),
        get_x=get_x,
        get_y=get_y,
        splitter=ColSplitter(col="is_valid")
 )

dls=MNIST.dataloaders(df)



def conv(ni, nf, ks=3, act=True):
      res = nn.Conv2d(ni, nf, stride=2, kernel_size=ks, padding=ks//2)
      if act: res = nn.Sequential(res, nn.ReLU())
      return res

def simple_cnn():
    return sequential(
            conv(1 ,4),  #14x14
            conv(4 ,8),  #7x7
            conv(8 ,16), #4x4
            conv(16,32), #2x2
            conv(32,47, act=False), #1x1
            Flatten(),
)

learn = Learner(dls, simple_cnn(), loss_func=F.cross_entropy, metrics=accuracy)

learn.summary()

learn.fit_one_cycle(2, 0.01)

def simple_cnn():
    return sequential(
            conv(1 ,16,ks=5),  #14x14
            conv(16 ,32),  #7x7
            conv(32 ,64), #4x4
            conv(64,128), #2x2
            conv(128,47, act=False), #1x1
            Flatten(),
)

learn.fit_one_cycle(1, 0.06)

dls=dblock.dataloaders(train_df,batch_size=512)

learn = Learner(dls, simple_cnn(), loss_func=F.cross_entropy, metrics=accuracy)

learn.fit_one_cycle(1, 0.06)

"""# BatchNormalization"""

def get_dls(bs=64):
 return DataBlock(
        blocks=(ImageBlock(cls=PILImageBW),CategoryBlock),
        get_x=get_x,
        get_y=get_y,
        batch_tfms=Normalize()
 ).dataloaders(df, bs=bs)

dls=get_dls(512)

def conv(ni, nf, ks=3, act=True):
 layers = [nn.Conv2d(ni, nf, stride=2, kernel_size=ks, padding=ks//2)]
 layers.append(nn.BatchNorm2d(nf))
 if act: layers.append(nn.ReLU())
 return nn.Sequential(*layers)

def simple_cnn():
    return sequential(
            conv(1 ,16,ks=5),  #14x14
            conv(16 ,32),  #7x7
            conv(32 ,64), #4x4
            conv(64,128), #2x2
            conv(128,47, act=False), #1x1
            Flatten(),
)

from fastai.callback.hook import *

learn = Learner(dls, simple_cnn(), loss_func=F.cross_entropy, metrics=accuracy)

learn.summary()

learn.fit_one_cycle(10,lr_max=0.1)



"""# Residual neural network"""

def get_x(r):
    return r.iloc[1:].to_numpy().reshape(28,28).transpose().astype(np.uint8)

def get_y(r):
    return r.iloc[0]

def get_dls(bs=64):
 return DataBlock(
        blocks=(ImageBlock(cls=PILImageBW),CategoryBlock),
        get_x=get_x,
        get_y=get_y,
        batch_tfms=Normalize()
 ).dataloaders(train_df, bs=bs)

dls=get_dls(512)

def block(ni, nf): return ConvLayer(ni, nf, stride=2)

doc(ConvLayer)



def get_model():
 return nn.Sequential(
 block(1, 16),
 block(16, 32),
 block(32, 64),
 block(64, 128),
 block(128, 256),
 nn.AdaptiveAvgPool2d(1),
 Flatten(),
 nn.Linear(256, dls.c))

learn=Learner(dls,get_model(), loss_func=nn.CrossEntropyLoss(), metrics=accuracy).to_fp16()

learn.lr_find()

learn.fit_one_cycle(5, 3e-3)



def _conv_block(ni,nf,stride):
      return nn.Sequential(
            ConvLayer(ni, nf, stride=stride),
            ConvLayer(nf, nf, act_cls=None, norm_type=NormType.BatchZero))

class ResBlock(Module):
      def __init__(self, ni, nf, stride=1):
            self.convs = _conv_block(ni,nf,stride)
            self.idconv = noop if ni==nf else ConvLayer(ni, nf, 1, act_cls=None)
            self.pool = noop if stride==1 else nn.AvgPool2d(2, ceil_mode=True)
      def forward(self, x):
            return F.relu(self.convs(x) + self.idconv(self.pool(x)))

doc(ResBlock)

def block(ni, nf):
 return nn.Sequential(ResBlock(ni=ni, nf=nf, stride=2), ResBlock(ni=nf, nf=nf))

def get_model():
 return nn.Sequential(
 block(1, 16),
 block(16, 32),
 block(32, 64),
 block(64, 128),
 block(128, 256),
 nn.AdaptiveAvgPool2d(1),
 Flatten(),
 nn.Linear(256, dls.c))

learn=Learner(dls,get_model(), loss_func=nn.CrossEntropyLoss(), metrics=accuracy).to_fp16()

learn.lr_find()

learn.fit_one_cycle(10, 3e-3)

"""# Cleaner"""

from fastai.vision.widgets import *

cleaner = ImageClassifierCleaner(learn)

cleaner